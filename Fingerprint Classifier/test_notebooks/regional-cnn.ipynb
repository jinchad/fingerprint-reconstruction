{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Faster R-CNN model with a ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Number of classes (your dataset classes + 1 for background)\n",
    "num_classes = 2  # For example, 2 classes + background\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the head of the model with a new one (for the number of classes in your dataset)\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(image_dir)\n",
    "            if f.endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(image_filename)[0] + \".txt\")\n",
    "\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Load labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    class_id, x_center, y_center, w, h = map(float, parts)\n",
    "\n",
    "                    # Convert from YOLO format to [x_min, y_min, x_max, y_max]\n",
    "                    x_center *= width\n",
    "                    y_center *= height\n",
    "                    w *= width\n",
    "                    h *= height\n",
    "\n",
    "                    x_min = x_center - w / 2\n",
    "                    y_min = y_center - h / 2\n",
    "                    x_max = x_center + w / 2\n",
    "                    y_max = y_center + h / 2\n",
    "\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(int(class_id) + 1)  # Make sure class starts from 1\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = CustomDataset(transforms=transform, image_dir= \"/Users/jin/Documents/GitHub/datasets/Fingerprint pattern/v1/standard_arch dataset v1/train/images\", label_dir=\"/Users/jin/Documents/GitHub/datasets/Fingerprint pattern/v1/standard_arch dataset v1/train/labels\")\n",
    "# Split into train and validation sets\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "valid_dataset = torch.utils.data.Subset(dataset, indices[-50:])\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, \n",
    "                                   collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, \n",
    "                                    collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.9924055604558242\n",
      "Epoch: 2, Loss: 0.5935266064970117\n",
      "Epoch: 3, Loss: 0.5256115245191675\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, \n",
    "                                                   weight_decay=0.0005)\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, \n",
    "                                                               gamma=0.1)\n",
    "# Train the model\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "   # Training loop\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {train_loss / len(train_loader)}')\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.3921,   2.9022, 239.8567,  93.2407],\n",
      "        [ 56.2282,  86.0404, 225.1559, 120.7215],\n",
      "        [ 56.2522,  65.8210, 224.5156, 100.2525],\n",
      "        [ 74.5040,  43.2841, 233.4493,  80.7620],\n",
      "        [ 67.3717, 105.8419, 233.5684, 141.1762],\n",
      "        [102.3403,  11.8732, 200.7585,  93.9527],\n",
      "        [ 84.3384,  72.7186, 237.8387, 112.1200],\n",
      "        [103.3822,  63.3907, 201.4525, 144.9570],\n",
      "        [ 68.3047,  23.0465, 229.4119,  59.9495],\n",
      "        [ 56.6734,   4.2122, 226.1692,  38.9015],\n",
      "        [ 20.4224,  61.7952, 256.0000, 137.3971],\n",
      "        [ 23.0091, 100.3154, 225.6618, 129.5054],\n",
      "        [ 43.7749,  19.2752, 256.0000,  46.6501],\n",
      "        [110.2022,   1.7493, 213.5853,  57.0366],\n",
      "        [ 33.0565, 120.5286, 242.4099, 149.7519],\n",
      "        [ 31.3008,  37.7520, 218.8332,  68.8878],\n",
      "        [ 79.0136,  38.7200, 193.6298, 127.6466],\n",
      "        [ 16.6159,  79.9667, 220.7029, 109.1118],\n",
      "        [ 97.2839, 134.3909, 256.0000, 171.8141],\n",
      "        [  0.0000, 101.2281, 256.0000, 173.3963],\n",
      "        [ 16.6445,  59.6394, 219.4194,  88.8073],\n",
      "        [133.5296,   7.3721, 256.0000, 149.3309]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 71.1813,  64.0298, 169.6520, 144.9210],\n",
      "        [ 37.4353,  74.2540, 197.8687, 111.3829],\n",
      "        [ 42.5979,  53.5721, 200.5456,  91.3711],\n",
      "        [ 44.3653, 125.7524, 207.8020, 162.4874],\n",
      "        [ 38.4837, 104.6202, 199.4219, 142.2849],\n",
      "        [ 60.9230, 145.9482, 222.0500, 183.0716],\n",
      "        [ 93.0947, 159.2728, 256.0000, 189.6204],\n",
      "        [ 72.3977, 118.9473, 256.0000, 149.0547],\n",
      "        [ 78.9990, 138.9791, 256.0000, 169.1012],\n",
      "        [ 70.2752,  98.5228, 256.0000, 129.0343],\n",
      "        [ 76.5749,  78.3501, 256.0000, 108.9590],\n",
      "        [ 47.2571,  33.0638, 232.4073,  70.9406],\n",
      "        [ 82.8523, 114.4163, 181.0017, 194.2665],\n",
      "        [ 29.4271,  25.9367, 205.9178,  59.5100],\n",
      "        [ 74.7024,  24.4694, 176.4705, 105.4526],\n",
      "        [ 69.8627, 165.9623, 233.0637, 203.0206],\n",
      "        [ 14.9126,  91.3171, 237.6802, 119.6767],\n",
      "        [  1.9545, 135.8855, 221.2379, 174.3411],\n",
      "        [ 12.1636,  71.0988, 243.3816,  99.4144],\n",
      "        [ 89.4716,  68.8358, 157.6771, 180.9061]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 39.4379,  90.7691, 256.0000, 119.2819],\n",
      "        [ 69.2546, 118.1568, 256.0000, 149.5135],\n",
      "        [ 42.0818,  74.8036, 203.6452, 111.2880],\n",
      "        [ 42.1132,  95.1730, 204.8820, 131.7099],\n",
      "        [  0.0000,  80.6623, 256.0000, 156.5637],\n",
      "        [ 81.4120,  73.8258, 179.9680, 154.8495],\n",
      "        [ 56.4910, 124.4503, 216.3871, 161.9116],\n",
      "        [ 28.3353,  70.3215, 245.4724,  98.8940],\n",
      "        [ 90.8193, 106.4109, 256.0000, 139.0648],\n",
      "        [  0.0000,  43.8059, 256.0000, 118.6656],\n",
      "        [ 52.2933,  48.1973, 256.0000,  79.4016],\n",
      "        [  6.4859, 119.5916, 200.9389, 150.3307],\n",
      "        [ 89.9022, 138.3708, 256.0000, 170.5667],\n",
      "        [ 44.9473,  54.5266, 208.4393,  91.1380],\n",
      "        [  0.0000, 120.7926, 256.0000, 192.5856],\n",
      "        [  0.0000, 109.8312, 196.3433, 140.1326],\n",
      "        [ 78.1632,  33.7498, 179.2635, 116.0353],\n",
      "        [ 98.8814,  55.1106, 256.0000,  88.8936],\n",
      "        [ 46.3991,  34.2745, 213.2216,  70.1787],\n",
      "        [109.5035, 126.7383, 256.0000, 160.2734]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 76.5888, 147.8606, 256.0000, 181.8123]])\n",
      "tensor([1])\n",
      "tensor([[ 18.7224, 166.4321, 185.9109, 202.6025],\n",
      "        [ 52.4112, 151.1822, 256.0000, 180.3217],\n",
      "        [ 57.9695, 125.4307, 158.0273, 206.4752],\n",
      "        [ 23.5648,  95.0340, 183.4866, 131.8446],\n",
      "        [ 40.9576, 141.7985, 256.0000, 170.0338],\n",
      "        [ 11.1369,  90.5946, 226.3593, 119.1881],\n",
      "        [  0.0000, 120.7579, 232.5206, 194.9637],\n",
      "        [  0.0000,  81.7213, 230.9857, 156.8524],\n",
      "        [ 20.2121,  79.8433, 228.1185, 109.1730],\n",
      "        [ 39.6567, 120.6507, 256.0000, 150.0762],\n",
      "        [ 33.9416, 132.2066, 255.0226, 160.1982],\n",
      "        [ 58.2623,  84.6455, 158.0193, 165.3990],\n",
      "        [  6.4982, 161.6188, 222.9405, 190.5970],\n",
      "        [ 52.2145, 169.9614, 256.0000, 200.0563],\n",
      "        [ 41.7797, 110.4871, 256.0000, 140.0040],\n",
      "        [  0.0000,  45.9925, 253.3155, 117.2799],\n",
      "        [ 80.4592,  56.4386, 256.0000,  88.4038],\n",
      "        [ 40.5490,  69.2848, 244.1555,  98.9615],\n",
      "        [ 24.6827, 180.2814, 237.5428, 210.1970],\n",
      "        [  1.0823, 148.6694, 173.9283, 182.0305],\n",
      "        [  0.4080, 118.1787, 168.9468, 151.0905]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 37.9836,  79.5221, 256.0000, 109.3408],\n",
      "        [ 36.7556,  69.3595, 256.0000,  99.1247],\n",
      "        [ 35.9183,  82.7207, 186.8656, 123.0227],\n",
      "        [ 70.3389,  27.3408, 256.0000,  58.9626],\n",
      "        [ 31.7719, 101.6916, 256.0000, 129.7197],\n",
      "        [  0.0000, 165.6322, 148.8821, 203.6254],\n",
      "        [ 50.9570,  84.2833, 148.6103, 165.2540],\n",
      "        [ 31.2318, 111.8550, 256.0000, 139.8686],\n",
      "        [ 33.5700,  49.0799, 256.0000,  78.9891],\n",
      "        [ 29.7583, 122.0634, 256.0000, 149.8332],\n",
      "        [ 48.3111,  53.0891, 202.1946,  91.9070],\n",
      "        [ 19.5475, 114.0324, 175.3105, 152.7414],\n",
      "        [  0.6763, 109.9437, 256.0000, 168.6431],\n",
      "        [ 14.1780, 134.4724, 168.4275, 173.5365],\n",
      "        [  0.0000, 161.9653, 215.8605, 190.2454],\n",
      "        [  0.0000, 149.4772, 256.0000, 204.3644],\n",
      "        [ 37.6760,  34.8403, 205.5966,  70.0005],\n",
      "        [ 31.0324, 142.0793, 256.0000, 169.8284],\n",
      "        [ 37.9191, 151.7615, 256.0000, 179.8293],\n",
      "        [  0.0000, 132.1624, 227.4955, 188.1106],\n",
      "        [ 26.2137, 136.2715, 136.4657, 219.9243],\n",
      "        [  1.3676,  62.8028, 220.4695, 136.0282],\n",
      "        [ 20.0183, 125.4224, 212.5382, 163.7404],\n",
      "        [ 59.5752,   3.5966, 256.0000,  96.5295],\n",
      "        [ 51.0603,  14.8178, 220.8806,  49.7570],\n",
      "        [ 71.7113,  35.0703, 184.9095, 118.1778],\n",
      "        [ 30.4896, 180.7028, 240.0045, 210.3018],\n",
      "        [ 65.5423,  90.8139, 134.2842, 201.9781]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n",
      "tensor([[ 53.3571,  54.1428, 214.4825,  90.7300],\n",
      "        [ 51.7623,  74.8576, 213.5264, 111.2296],\n",
      "        [ 85.9914, 124.7152, 242.3125, 162.8314],\n",
      "        [112.2279, 114.4483, 210.7977, 195.1344],\n",
      "        [ 87.4979,  43.9737, 188.8038, 124.9141],\n",
      "        [ 93.9862,  74.8427, 196.4148, 154.6447],\n",
      "        [ 45.7830,  96.4322, 214.6046, 131.0964],\n",
      "        [ 94.3147, 144.3232, 246.9598, 183.4751],\n",
      "        [ 78.4061,  67.7765, 256.0000,  97.9570],\n",
      "        [ 75.2645, 157.2387, 240.5425, 192.8123],\n",
      "        [ 51.8159,  34.9816, 217.7196,  70.2303],\n",
      "        [ 49.8493,  49.6638, 256.0000,  77.6073],\n",
      "        [ 61.3304, 110.3242, 256.0000, 139.0517],\n",
      "        [ 51.8857, 170.7882, 244.9589, 201.5803],\n",
      "        [ 34.3112,  91.1962, 256.0000, 117.8176]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 68.5832,  56.7151, 256.0000,  88.1770],\n",
      "        [ 59.4718,  42.9680, 217.0247,  81.1991],\n",
      "        [ 37.6283,  39.8696, 256.0000,  68.6282],\n",
      "        [ 47.3698,  27.5727, 256.0000,  58.4036],\n",
      "        [ 67.8169,  87.5659, 256.0000, 118.4924],\n",
      "        [ 81.8803,  53.1454, 180.4589, 134.7490],\n",
      "        [ 48.8610,  63.7554, 209.6051, 100.9243],\n",
      "        [  6.5166,  23.3107, 256.0000,  99.2795],\n",
      "        [ 85.6019, 107.1598, 256.0000, 138.9350],\n",
      "        [ 48.3821, 100.4910, 256.0000, 129.5466],\n",
      "        [ 28.7638,  80.7795, 242.6560, 109.0196],\n",
      "        [  0.0000,  73.0057, 234.3178, 131.2816],\n",
      "        [ 53.8730,  14.9308, 223.7069,  49.2722],\n",
      "        [ 64.1714,   4.7435, 236.0169,  39.1876],\n",
      "        [  0.5315,  54.2570, 223.3165, 111.9248],\n",
      "        [100.8540, 116.8492, 256.0000, 149.3203],\n",
      "        [ 87.5238,   3.9774, 219.8623, 118.3406],\n",
      "        [116.0340,   4.0015, 256.0000,  91.2583],\n",
      "        [  9.9832,  58.0000, 196.5637,  89.3189],\n",
      "        [ 74.7518,  21.6412, 175.7925, 105.0232]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 62.7187, 120.1852, 256.0000, 149.3530],\n",
      "        [ 79.8815,  88.7675, 256.0000, 119.4785],\n",
      "        [ 41.3745,   2.9808, 205.2126,  39.6693],\n",
      "        [ 89.2405, 107.2045, 256.0000, 139.6155],\n",
      "        [ 49.8669,  52.8169, 206.3004,  91.3870],\n",
      "        [ 54.8130,  72.9688, 209.7389, 111.8310],\n",
      "        [ 75.0862, 128.8815, 256.0000, 159.6225],\n",
      "        [ 58.1849,  64.9502, 256.0000, 102.1596],\n",
      "        [ 65.9765,   3.8382, 197.0338, 136.2254],\n",
      "        [ 55.8725,  94.1092, 237.2610, 132.7184],\n",
      "        [ 77.7226,  53.7626, 177.6514, 134.6941],\n",
      "        [ 48.8957,  32.2628, 206.1453,  70.9546],\n",
      "        [ 55.2718,  12.4556, 238.3132,  50.7887],\n",
      "        [ 77.0219,   2.2134, 177.6279,  73.8407],\n",
      "        [ 57.0879,  44.1419, 256.0000,  81.9520],\n",
      "        [  1.6197,  35.4381, 220.3560,  92.6644],\n",
      "        [ 68.6252,  29.3677, 256.0000,  55.5370],\n",
      "        [  2.3119,  54.5652, 215.7862, 111.9164],\n",
      "        [ 97.1900,  18.4764, 165.3184, 130.0636],\n",
      "        [  1.5009,   4.2364, 245.6852,  78.1509],\n",
      "        [  2.4405,  73.2212, 222.2091, 131.2134]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[1.7716e+01, 1.5442e+02, 1.7528e+02, 1.9320e+02],\n",
      "        [6.6483e+00, 1.7176e+02, 2.1911e+02, 2.0102e+02],\n",
      "        [1.6666e+01, 1.3528e+02, 2.0222e+02, 1.7417e+02],\n",
      "        [1.1442e+01, 1.5216e+02, 2.3146e+02, 1.8026e+02],\n",
      "        [2.0703e+01, 1.1192e+02, 2.5600e+02, 1.4033e+02],\n",
      "        [3.1777e+01, 8.3251e+01, 1.2877e+02, 1.6505e+02],\n",
      "        [3.3362e+01, 6.9743e+01, 2.5600e+02, 9.9343e+01],\n",
      "        [2.6401e+01, 8.0156e+01, 2.5600e+02, 1.0939e+02],\n",
      "        [2.1282e+01, 1.0119e+02, 2.5600e+02, 1.3009e+02],\n",
      "        [2.2910e+01, 9.0766e+01, 2.5600e+02, 1.1967e+02],\n",
      "        [4.2600e+01, 5.9708e+01, 2.5600e+02, 8.9142e+01],\n",
      "        [3.0829e+01, 1.8222e+02, 2.5600e+02, 2.1021e+02],\n",
      "        [1.4742e+01, 1.1499e+02, 2.0087e+02, 1.5400e+02],\n",
      "        [4.2934e+01, 1.2564e+02, 1.5701e+02, 2.0973e+02],\n",
      "        [3.0311e+01, 2.7607e+01, 2.3829e+02, 5.8888e+01],\n",
      "        [3.3374e+01, 3.8272e+01, 2.4005e+02, 6.8741e+01],\n",
      "        [1.5003e+00, 1.5452e+02, 2.5600e+02, 2.0486e+02],\n",
      "        [5.6035e-02, 1.1956e+02, 2.2324e+02, 1.9421e+02],\n",
      "        [2.5562e+01, 4.9231e+01, 2.3353e+02, 7.8819e+01],\n",
      "        [5.0454e+00, 1.7319e+01, 2.0248e+02, 4.9282e+01],\n",
      "        [5.9331e+01, 1.6008e+02, 2.5600e+02, 1.9034e+02],\n",
      "        [3.3634e+01, 4.1682e+01, 1.2997e+02, 1.2530e+02],\n",
      "        [2.5639e+00, 5.5991e+01, 2.5124e+02, 1.1317e+02],\n",
      "        [3.2366e+00, 3.4795e+01, 2.5600e+02, 9.3538e+01],\n",
      "        [4.1321e-01, 9.5730e+01, 2.4088e+02, 1.5145e+02],\n",
      "        [1.9128e+00, 1.2470e+02, 1.5842e+02, 1.6240e+02],\n",
      "        [1.1719e+01, 5.2583e+01, 1.6444e+02, 9.1538e+01]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1])\n",
      "tensor([[ 55.8793, 126.1507, 156.8008, 205.9864]])\n",
      "tensor([1])\n",
      "tensor([[ 52.1287,  98.5653, 256.0000, 128.5322],\n",
      "        [ 50.0670,  88.5434, 256.0000, 118.1610],\n",
      "        [ 49.0456,  68.0222, 256.0000,  97.5629],\n",
      "        [ 55.1393,  53.2372, 212.5688,  91.1609],\n",
      "        [ 47.0279, 136.4476, 215.1455, 171.8225],\n",
      "        [ 49.5972,  78.3113, 256.0000, 107.7380],\n",
      "        [ 54.4197, 108.3723, 256.0000, 138.5549],\n",
      "        [ 49.7495,  37.4001, 256.0000,  67.6869],\n",
      "        [ 28.5183,  50.7803, 249.7616,  79.0313],\n",
      "        [ 39.5855, 131.6221, 256.0000, 159.9551],\n",
      "        [ 92.7444, 147.9977, 256.0000, 180.1454],\n",
      "        [ 80.8753,  52.9212, 180.0639, 135.4923],\n",
      "        [ 46.3028, 115.2986, 207.9887, 152.1150],\n",
      "        [ 80.2554,  84.0644, 179.2905, 164.8459],\n",
      "        [ 69.4330,  22.6441, 226.5679,  60.9208],\n",
      "        [ 71.4549,   3.8219, 201.2085, 119.7284],\n",
      "        [ 82.3627, 159.8696, 256.0000, 190.6939],\n",
      "        [ 47.4809,   5.9337, 229.1373,  38.4393],\n",
      "        [  8.6466,  23.2013, 256.0000,  99.6270],\n",
      "        [  0.8219,  99.7285, 256.0000, 176.0023],\n",
      "        [ 34.6214,  17.0299, 223.4881,  48.4005],\n",
      "        [  0.9498,  61.8622, 254.9875, 137.4882],\n",
      "        [100.5977,   2.3211, 205.9811,  66.3889],\n",
      "        [ 15.6096, 146.0862, 208.6628, 184.0497],\n",
      "        [  1.9697,   1.4208, 256.0000,  59.1988]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1])\n",
      "tensor([[ 55.9647, 166.5974, 218.3615, 202.5416],\n",
      "        [ 33.2305, 177.5216, 216.4905, 212.3080],\n",
      "        [101.8262, 114.6191, 200.2792, 195.4166],\n",
      "        [101.4835,  73.7091, 200.9448, 156.3865],\n",
      "        [ 62.1361, 115.8095, 224.0369, 152.0701],\n",
      "        [ 72.7787, 155.3882, 228.4132, 193.1857],\n",
      "        [ 80.9402, 134.0674, 235.1079, 173.4429],\n",
      "        [ 68.6386,  95.0291, 229.0182, 132.0938],\n",
      "        [ 73.0793,  74.2583, 231.1672, 112.0977],\n",
      "        [ 94.8171,  41.4311, 195.2075, 124.9772],\n",
      "        [ 38.0325, 128.3703, 214.2391, 161.4303],\n",
      "        [ 41.2860, 148.8185, 216.2473, 181.9345],\n",
      "        [ 94.5668,  67.5696, 256.0000,  98.5408],\n",
      "        [ 62.3386, 110.2537, 256.0000, 138.5851],\n",
      "        [ 76.4333,  90.2922, 188.7040, 178.6639],\n",
      "        [ 74.1814, 132.2264, 184.8562, 220.8740],\n",
      "        [ 80.6237,  53.4217, 236.2861,  91.7254]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Test on a new image\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        predictions = model(images)\n",
    "        # Example: print the bounding boxes and labels for the first image\n",
    "        print(predictions[0]['boxes'])\n",
    "        print(predictions[0]['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 53.7582,   5.4758, 268.0063,  49.9969],\n",
      "        [ 53.0059,   3.3054, 283.5370, 119.3975],\n",
      "        [ 65.3791,  57.4891, 274.7841, 102.3167],\n",
      "        [117.0816,   3.7758, 241.6418, 105.1724],\n",
      "        [ 87.4906,  14.9046, 285.4268,  64.2436],\n",
      "        [ 92.5538,  40.8661, 287.7224,  90.0456],\n",
      "        [ 77.0517,  82.6359, 283.1801, 128.2363],\n",
      "        [121.6125,  43.5112, 251.1219, 145.1951],\n",
      "        [ 48.7955,  32.1668, 264.3596,  75.7324],\n",
      "        [ 78.1005, 108.7204, 283.0779, 154.5441]])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Load image\n",
    "img = Image.open(\"test_images/arch/arch_3.tif\")\n",
    "\n",
    "# Apply the same transformation as for training\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(device)\n",
    "# Model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(img)\n",
    "scores = prediction[0]['scores']\n",
    "\n",
    "threshold = 0.03\n",
    "conf_mask = scores > threshold\n",
    "\n",
    "# Print the predicted bounding boxes and labels\n",
    "print(prediction[0]['boxes'][conf_mask])\n",
    "print(prediction[0]['labels'][conf_mask])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
